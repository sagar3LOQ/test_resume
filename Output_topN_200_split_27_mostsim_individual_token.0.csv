matthews_corrcoef	roc_auc_score	precision[0]	precision[1]	recall[0]	recall[1]	fscore[0]	fscore[1]	support_0	support_1
-0.0379276147	0.4941860465	0.8762886598	0	0.988372093	0	0.9289617486	0	86	12
-0.0235427755	0.4946236559	0.9484536082	0	0.9892473118	0	0.9684210526	0	93	5
0	0.5	0.9387755102	0	1	0	0.9684210526	0	92	6
0.2855468783	0.5454545455	0.8969072165	1	1	0.0909090909	0.9456521739	0.1666666667	87	11
0.3012003478	0.55	0.9072164948	1	1	0.1	0.9513513514	0.1818181818	88	10
0.3192918855	0.5555555556	0.9175257732	1	1	0.1111111111	0.9569892473	0.2	89	9
0.3314700715	0.5943181818	0.9157894737	0.6666666667	0.9886363636	0.2	0.9508196721	0.3076923077	88	10
0.3537105525	0.6054931336	0.9263157895	0.6666666667	0.9887640449	0.2222222222	0.9565217391	0.3333333333	89	9
0.4281744193	0.6	0.9166666667	1	1	0.2	0.9565217391	0.3333333333	88	10
0.4997607083	0.6363636364	0.9157894737	1	1	0.2727272727	0.956043956	0.4285714286	87	11
									
									
									
Algo: Find topN 200 positive words based on tfidf model and M=200 random negative words from word2vec model that excludes the words occured in positive words, documents and top 10 words most similar to positive words. Then both positive and negative words are used to generate doc vector using logistic regression coefficient and then this doc vectors are again given to LR model for training and prediction of CVs									
